
 Lab 4 - Working with ADLS-gen2
 -------------------------------
 Ref URL: https://learn.microsoft.com/en-us/azure/databricks/storage/azure-storage
 
 
 1. Login to the Azure portal and make sure you have the following resources
    (Create them if not already there)
	
	1.1 A resource group (name: cts-demo-rg)
	1.2 A storage account (name: ctsdemosa) within the above resource group
	1.3 An Azure Databricks workspace (cts-demo-premium-eastus-ws)
			Pricing tier: Standard
			Region: East US
			
 2. Open the storage account and upload sample data into a container
 
	2.1 Open the Storage account page from the Azure portal
	2.2 Click on 'Containers' menu and click on '+ Container' to add.
		name: demo
	2.3 Open the container and click on 'Upload' to add the following file.
		file: circuits.csv
		
 3. Disable 'soft delete' for blobs and containers
 
	3.1 Open the Storage account page from the Azure portal
	3.2 Click on 'Data protection' option under 'Data management'
	3.3 Uncheck the following:
		Uncheck: Enable soft delete for blobs
		Uncheck: Enable soft delete for containers
	3.4 Click on 'Save' to save the changes
  
 
 4. Open your Databricks workspace and create an 'All purpose' cluster. 
		name: CTS Demo Cluster
		Single node 
		Access mode: Single user
		Databricks runtime version: 13.3 LTS
		Uncheck 'Use Photon Acceleration'
		Node type: Standard_F4
		Terminate after 20 minutes of inactivity
		
		** Your cost should be 0.5 DBU/hour
		
		
		
 Lab 4a: Access ADLS using access keys
 -------------------------------------

 5. Get the 'Access key' of your storage account. 
 
	5.1 Open you storage account and click on 'Access keys' menu option (under 'Security + networking')
		There will be two keys here. You can use 'key 1'
		
	5.2 Click on 'Show' button of the 'Key' and copy the key.
		This is the access key of your storage account
	
 6. Import the following notebook and run it.
	Make sure to change 'storage account' and 'access key' as in your case.
	
	Notebook: ADLS-using-Access-Keys.dbc
	
	
	
 Lab 4b: Access ADLS using SAS token
 -----------------------------------
	SAS -> Shared Access Signature
	
 7. Generate the SAS token for your container
 
	7.1 Open your storage account and then your container from Azure portal
	7.2 Click on context menu (... icon) of your container and select 'Generate SAS'
	7.3 Under permissions dropdown select 'Read' and 'List'
	7.4 Choose appropriate time period (Start and End times for the validity of the token)
	7.5 Click on 'Generate SAS token and URL' button
	7.6 Copy and save the 'Blob SAS token' and save it somewhere. 
	
 8. Import the following notebook and run it.
	Make sure to change 'storage account' and 'SAS token' as in your case.
	
	Notebook: ADLS-using-SAS-token.dbc	
		
	
 Lab 4c: Access ADLS using Service Principal
 -------------------------------------------
  => A Service Principal is like a user account that can be registered in the Entra ID (Active Directory)
     SPs are assigned permissions to access resources in your Azure subscription via RBAC (role-based access control). 
	
 9. Register Microsoft Entra ID Application / Service Principal 
    NOTE: Microsoft Entra ID is formerly known as Microsoft Active Directory
	
	9.1 Search for and open 'Microsoft Entra ID' and Click on 'Manage' -> 'App registrations' menu option.
	9.2 Click on '+ New registration' and create an app as below:
		name: cts-adls-using-sp-app
	9.3 Click on 'Register' button
	    This creates a Service Principal for us. 
	9.4 Make a note of the following details from the app registration page.
	
			Application (client) ID: <Application (client) ID>
			Directory (tenant) ID: <Directory (tenant) ID>
			
 10. Generate a secret/password for the Application
 
	10.1 Click on 'Manage' -> 'Certificates & secrets' on your app registration page.
	10.2 Click on '+ New client secret' button. 
	10.3 Create a secret as follows:
		Description: CTS ADLS Using Service Principal
		** DO NOT NAVIGATE AWAY FROM THIS PAGE UNTIL YOU COPY REQUIRED INFO **
	10.4 Make a note of the secret value (value, not id)
	
		Client Secret Value: <VALUE>
		
 11. Assign Role 'Storage Blob Data Contributor' to the Microsoft Entra ID Application.
 
	11.1 Open your storage account page from Azure portal
	11.2 Click on 'Access Control (IAM)' option
	11.3 Click on 'Add' -> 'Add role assignment'
	11.4 Search for and select 'Storage Blob Data Contributor'. Click 'Next'
	11.5 Click on '+ Select members'
	11.6 Search for your service principal (i.e cts-adls-using-sp-app) and add it.
	11.7 Click on 'Review + assign' and 'Review + assign' again 
	     
	** This adds 'Storage Blob Contributor' role on 'storage account' to 'service principal'. 
		
 12. Import the following notebook and run it.
	 Make sure to change 'storage account' and other variables as appropriate.
	
	Notebook: ADLS-using-Service-Principal.dbc	
		

 	
 Lab 4d: Access ADLS using Cluster scoped credentials (Access key)
 -----------------------------------------------------------------
 
 13. Open the All-purpose cluster page and click on 'Edit'
 
 14. Add the following line under 'Advanced options' -> 'Spark config'
	 Replace the place-holders in the string
 
	fs.azure.account.key.<storage-account>.dfs.core.windows.net <access-key>
	
	NOTE:
	
	In case you are using secret scope change the above line as follows:	
	fs.azure.account.key.<storage-account>.dfs.core.windows.net {{secrets/<secret-scope>/<secret>}}
	
	example:
	fs.azure.account.key.ctsdemosa.dfs.core.windows.net {{secrets/cts-demo-scope/cts-demo-sa-access-key}}
	
	
 15. Click on 'Confirm and restart' button.
 
 16. Import the following notebook and run it.
	 Make sure to change 'storage account' and other variables as appropriate.
	
	Notebook: ADLS-using-Cluster-Scoped-Credentials.dbc		
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	